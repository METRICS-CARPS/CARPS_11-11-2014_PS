---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

# Report Details

```{r}
articleID <- "11-11-2014" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- "final" # specify whether this is the 'pilot' report or 'final' report
pilotNames <- "Tom Hardwicke" # insert the pilot's name here e.g., "Tom Hardwicke".  If there are multiple cpilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "Michael Frank" # # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- 150 # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- 180 # insert the co-pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- as.Date("06/13/18", format = "%m/%d/%y") # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- as.Date("10/18/18", format = "%m/%d/%y") # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- as.Date("04/20/19", format = "%m/%d/%y") # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

------

#### Methods summary: 

The authors examined whether sharing a painful experience with other people in a small group might promote bonding more than sharing a similar but nonpainful social experience. 54 Participants were randomly allocated to either a pain condition (n = 27) or a no-pain condition (n = 27). Participants were asked to rate their feeling of bonding to the other participants and the physical pain of the tasks by rating statements on a likert scale.

------

#### Target outcomes: 

> Manipulation checks revealed that reported pain intensity
was higher in the pain condition (M = 6.07, SD =
1.99) than in the no-pain condition (M = 1.67, SD = 0.92),
t(52) = 10.41, p < .001. Reported unpleasantness was also
greater in the pain condition (M = 6.00, SD = 1.96) than
in the no-pain condition (M = 1.74, SD = 1.19), t(52) =
9.63, p = .001. There were no significant differences
between conditions in positive affect (pain condition:
M = 3.05, SD = 0.82; no-pain condition: M = 2.80, SD =
0.83), t(52) = 1.09, p = .283, or negative affect (pain condition:
M = 1.34, SD = 0.45; no-pain condition: M = 1.27,
SD = 0.37), t(52) = 0.60, p = .554. Compared with the
control tasks, the pain tasks were viewed as marginally
more threatening (pain tasks: M = 1.36, SD = 0.58; control
tasks: M = 1.11, SD = 0.30), t(52) = 1.97, p = .054, but not
more challenging (pain tasks: M = 2.67, SD = 0.87; control
tasks: M = 2.37, SD = 0.91), t(52) = 1.22, p = .227.
We predicted that participants who shared a painful
experience, compared with those who shared a similar
but nonpainful social experience, would feel more
bonded together. A one-way analysis of variance
(ANOVA) revealed that pain had a medium-sized effect
on bonding, F(1, 52) = 4.09, p = .048, d = 0.54 (see Fig. 1);
participants in the pain condition reported higher bonding
(M = 3.71, SD = 1.01, 95% confidence interval, or CI =
[3.33, 4.09]) than did those in the no-pain condition (M =
3.14, SD = 1.09, 95% CI = [2.73, 3.55]).

------

```{r global_options, include=FALSE}
# sets up some formatting options for the R Markdown document
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Step 1: Load packages and prepare report object

```{r}
# load packages
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
library(ez) # for anova
library(lsr) # for cohens d
```

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

# Step 2: Load data

```{r}
d <- read_sav("data/Study_1_Data.sav")
```

# Step 3: Tidy data

Let's check we have 27 in each condition:

```{r}
d %>% count(condition)
```
We have 27 in each condition but the condition labels are not identified i.e., we don't know which is pain and which is no pain. After calculating the group means (below), we can see that 0 = no pain, 1 = pain. 

# Step 4: Run analysis

> Manipulation checks revealed that reported pain intensity
was higher in the pain condition (M = 6.07, SD =
1.99) than in the no-pain condition (M = 1.67, SD = 0.92),

```{r}
desc_out <- d %>% 
  group_by(condition) %>% 
  summarise(M = mean(task_intense), 
            SD = sd(task_intense))
```

```{r}
reportObject <- reproCheck(reportedValue = "6.07", obtainedValue = filter(desc_out, condition == 1) %>% select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "1.99", obtainedValue = filter(desc_out, condition == 1) %>% select(SD), valueType = 'sd')

reportObject <- reproCheck(reportedValue = "1.67", obtainedValue = filter(desc_out, condition == 0) %>% select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "0.92", obtainedValue = filter(desc_out, condition == 0) %>% select(SD), valueType = 'sd')
```
> t(52) = 10.41, p < .001

```{r}
t.out <- t.test(filter(d, condition == 1) %>% 
                  pull(task_intense), filter(d, condition == 0) %>% 
                  pull(task_intense), paired = F, var.equal = T)
```

```{r}
reportObject <- reproCheck(reportedValue = "52", 
                           obtainedValue = t.out$parameter, valueType = 'df')
reportObject <- reproCheck(reportedValue = "10.41", 
                           obtainedValue = t.out$statistic, valueType = 't')
reportObject <- reproCheck(reportedValue = "<.001", 
                           obtainedValue = t.out$p.value, valueType = 'p', 
                           eyeballCheck = TRUE)
```

> Reported unpleasantness was also greater in the pain condition (M = 6.00, SD = 1.96) than
in the no-pain condition (M = 1.74, SD = 1.19), 

```{r}
desc_out <- d %>% 
  group_by(condition) %>% 
  summarise(M = mean(task_unpleasant), SD = sd(task_unpleasant))
```

```{r}
reportObject <- reproCheck(reportedValue = "6.00", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "1.96", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(SD), valueType = 'sd')

reportObject <- reproCheck(reportedValue = "1.74", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "1.19", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(SD), valueType = 'sd')
```

> t(52) = 9.63, p = .001. 

```{r}
t.out <- t.test(filter(d, condition == 1) %>% 
                  pull(task_unpleasant), filter(d, condition == 0) %>% 
                  pull(task_unpleasant), paired = F, var.equal = T)
```

```{r}
reportObject <- reproCheck(reportedValue = "52",
                           obtainedValue = t.out$parameter, valueType = 'df')
reportObject <- reproCheck(reportedValue = "9.63", 
                           obtainedValue = t.out$statistic, valueType = 't')
reportObject <- reproCheck(reportedValue = ".001", 
                           obtainedValue = t.out$p.value, valueType = 'p')
```

>There were no significant differences between conditions in positive affect (pain condition: M = 3.05, SD = 0.82; no-pain condition: M = 2.80, SD = 0.83), 

```{r}
desc_out <- d %>% group_by(condition) %>% 
  summarise(M = mean(Pos_PANAS), 
            SD = sd(Pos_PANAS))
```

```{r}
reportObject <- reproCheck(reportedValue = "3.05", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "0.82", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(SD), valueType = 'sd')

reportObject <- reproCheck(reportedValue = "2.80", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "0.83", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(SD), valueType = 'sd')
```


> t(52) = 1.09, p = .283, 

```{r}
t.out <- t.test(filter(d, condition == 1) %>% 
                  pull(Pos_PANAS), filter(d, condition == 0) %>% 
                  pull(Pos_PANAS), paired = F, var.equal = T)
```

```{r}
reportObject <- reproCheck(reportedValue = "52", 
                           obtainedValue = t.out$parameter, 
                           valueType = 'df')
reportObject <- reproCheck(reportedValue = "1.09", 
                           obtainedValue = t.out$statistic, 
                           valueType = 't')
reportObject <- reproCheck(reportedValue = ".283", 
                           obtainedValue = t.out$p.value, 
                           valueType = 'p')
```

> or negative affect (pain condition: M = 1.34, SD = 0.45; no-pain condition: M = 1.27, SD = 0.37), 

```{r}
desc_out <- d %>% 
  group_by(condition) %>% 
  summarise(M = mean(Neg_PANAS), SD = sd(Neg_PANAS))
```

```{r}
reportObject <- reproCheck(reportedValue = "1.34", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "0.45", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(SD), valueType = 'sd')

reportObject <- reproCheck(reportedValue = "1.27", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "0.37", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(SD), valueType = 'sd')
```

> t(52) = 0.60, p = .554. 

```{r}
t.out <- t.test(filter(d, condition == 1) %>% 
                  pull(Neg_PANAS), filter(d, condition == 0) %>% 
                  pull(Neg_PANAS), paired = F, var.equal = T)
```

```{r}
reportObject <- reproCheck(reportedValue = "52", 
                           obtainedValue = t.out$parameter, 
                           valueType = 'df')
reportObject <- reproCheck(reportedValue = "0.60", 
                           obtainedValue = t.out$statistic, 
                           valueType = 't')
reportObject <- reproCheck(reportedValue = ".554", 
                           obtainedValue = t.out$p.value, 
                           valueType = 'p')
```

> Compared with the control tasks, the pain tasks were viewed as marginally more threatening (pain tasks: M = 1.36, SD = 0.58; control tasks: M = 1.11, SD = 0.30), 

```{r}
desc_out <- d %>% 
  group_by(condition) %>% 
  summarise(M = mean(Threat_TOT), 
            SD = sd(Threat_TOT))
```

```{r}
reportObject <- reproCheck(reportedValue = "1.36", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "0.58", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(SD), valueType = 'sd')

reportObject <- reproCheck(reportedValue = "1.11", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "0.30", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(SD), valueType = 'sd')
```

> t(52) = 1.97, p = .054, 

```{r}
t.out <- t.test(filter(d, condition == 1) %>% 
                  pull(Threat_TOT), filter(d, condition == 0) %>% 
                  pull(Threat_TOT), paired = F, var.equal = T)
```

```{r}
reportObject <- reproCheck(reportedValue = "52", 
                           obtainedValue = t.out$parameter, valueType = 'df')
reportObject <- reproCheck(reportedValue = "1.97", 
                           obtainedValue = t.out$statistic, valueType = 't')
reportObject <- reproCheck(reportedValue = ".054", 
                           obtainedValue = t.out$p.value, valueType = 'p')
```

> but not more challenging (pain tasks: M = 2.67, SD = 0.87; control tasks: M = 2.37, SD = 0.91), 

```{r}
desc_out <- d %>% 
  group_by(condition) %>% 
  summarise(M = mean(Challenge_TOT), SD = sd(Challenge_TOT))
```

```{r}
reportObject <- reproCheck(reportedValue = "2.67", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "0.87", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(SD), valueType = 'sd')

reportObject <- reproCheck(reportedValue = "2.37", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "0.91", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(SD), valueType = 'sd')
```

> t(52) = 1.22, p = .227.

```{r}
t.out <- t.test(filter(d, condition == 1) %>% 
                  pull(Challenge_TOT), filter(d, condition == 0) %>% 
                  pull(Challenge_TOT), paired = F, var.equal = T)
```

```{r}
reportObject <- reproCheck(reportedValue = "52", 
                           obtainedValue = t.out$parameter, valueType = 'df')
reportObject <- reproCheck(reportedValue = "1.22", 
                           obtainedValue = t.out$statistic, valueType = 't')
reportObject <- reproCheck(reportedValue = ".227", 
                           obtainedValue = t.out$p.value, valueType = 'p')
```

> We predicted that participants who shared a painful experience, compared with those who shared a similar but nonpainful social experience, would feel more bonded together. A one-way analysis of variance (ANOVA) revealed that pain had a medium-sized effect on bonding, F(1, 52) = 4.09, p = .048, d = 0.54 (see Fig. 1); 

```{r}
aov_data <- d %>% select(subid, condition, GroupCohension_TOT)

aov_out <- ezANOVA(aov_data, wid = subid, 
                   dv = GroupCohension_TOT , within = NULL, 
                   between = condition , observed = NULL , diff = NULL, 
                   reverse_diff = FALSE , type = 2)

d.out <- cohensD(filter(aov_data, condition == 0) %>% 
                   pull(GroupCohension_TOT), filter(aov_data, condition == 1) %>% 
                   pull(GroupCohension_TOT))
```

```{r}
reportObject <- reproCheck(reportedValue = "1", 
                           obtainedValue = aov_out$ANOVA$DFn, valueType = 'df')
reportObject <- reproCheck(reportedValue = "52", 
                           obtainedValue = aov_out$ANOVA$DFd, valueType = 'df')
reportObject <- reproCheck(reportedValue = "0.048", 
                           obtainedValue = aov_out$ANOVA$p, valueType = 'p')
reportObject <- reproCheck(reportedValue = "0.54", 
                           obtainedValue = d.out, valueType = 'd')
```

> participants in the pain condition reported higher bonding (M = 3.71, SD = 1.01, 95% confidence interval, or CI = [3.33, 4.09]) than did those in the no-pain condition (M = 3.14, SD = 1.09, 95% CI = [2.73, 3.55]).

```{r}
desc_out <- d %>% group_by(condition) %>% 
  summarise(M = mean(GroupCohension_TOT), SD = sd(GroupCohension_TOT))
```

```{r}
reportObject <- reproCheck(reportedValue = "3.71", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "1.01", 
                           obtainedValue = filter(desc_out, condition == 1) %>% 
                             select(SD), valueType = 'sd')

ci.out <- t.test(filter(aov_data, condition == 1) %>% 
                   pull(GroupCohension_TOT))$conf.int

reportObject <- reproCheck(reportedValue = "3.33", 
                           obtainedValue = ci.out[1], valueType = 'ci')
reportObject <- reproCheck(reportedValue = "4.09", 
                           obtainedValue = ci.out[2], valueType = 'ci')


reportObject <- reproCheck(reportedValue = "3.14", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(M), valueType = 'mean')

reportObject <- reproCheck(reportedValue = "1.09", 
                           obtainedValue = filter(desc_out, condition == 0) %>% 
                             select(SD), valueType = 'sd')

ci.out <- t.test(filter(aov_data, condition == 0) %>% 
                   pull(GroupCohension_TOT))$conf.int

reportObject <- reproCheck(reportedValue = "2.73", 
                           obtainedValue = ci.out[1], valueType = 'ci')
reportObject <- reproCheck(reportedValue = "3.55", 
                           obtainedValue = ci.out[2], valueType = 'ci')
```

# Step 5: Conclusion

This reproducibility check was largely a success. There were some difficulties mapping the reported analyses to the variable names in the data file, however these were resolved with a bit of guesswork.

There was one major error where a p value was reported as = .001 but we obtained 3.695211e-13. It seems very likely the authors meant to report p <.001 as the reported t-value is consistent with this significance level (the authors have confirmed over e-mail that it was probably a typo).

```{r}
Author_Assistance = TRUE # was author assistance provided? (if so, enter TRUE)

Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?

# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).

locus_typo <- 1 # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- 0 # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- 0 # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- 0 # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- 0 # how many discrete issues were there for which you could not identify the cause

Affects_Conclusion <- FALSE # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? TRUE, FALSE, or NA. This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add variables to report 
  select(articleID, everything()) # make articleID first column

# decide on final outcome
if(any(reportObject$comparisonOutcome != "MATCH") | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}

# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified)

# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}

if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
